{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adityacse1001/Fake-News-Detection/blob/main/finaladitya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfaMjp98d1Cl",
        "outputId": "88d38c54-95ce-4b3b-9f65-8ae50c121d4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFhZjch0dsXr",
        "outputId": "951d2433-7f86-4285-bbc2-9c18b993566f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_bi7e8q6d0VW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgUoLbL8ebTG",
        "outputId": "ba9d5b5f-0fec-4ab6-f4b2-5a2c49f5d936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZEJ5mfLceqUe"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the datasets\n",
        "true_data = pd.read_csv('/content/drive/MyDrive/True.csv')\n",
        "fake_data = pd.read_csv('/content/drive/MyDrive/Fake.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nl6IS_H9exzx"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Add a label column to indicate whether the news is true (1) or fake (0)\n",
        "true_data['label'] = 1\n",
        "fake_data['label'] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6pARW5-QeybV"
      },
      "outputs": [],
      "source": [
        "# Concatenate the datasets\n",
        "data = pd.concat([true_data, fake_data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HLEJo1mre2tN"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Removing punctuation and non-alphabetic characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenizing\n",
        "    tokens = text.split()\n",
        "    # Removing stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "    # Apply preprocessing to the text data\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WAgHxrPofBa1"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Tokenize the text and convert it into sequences\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hF40JT8TfJ6Y"
      },
      "outputs": [],
      "source": [
        "# Pad the sequences to have the same length\n",
        "max_sequence_length = 1000  # choose an appropriate value\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RE8EAzscfgQh",
        "outputId": "48d677ea-70e7-462e-ceb4-70aee91e60a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define the model\n",
        "embedding_dim = 100  # choose an appropriate value\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(tokenizer.word_index) + 1, embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1rniAMQfhHi",
        "outputId": "ab7ed230-8464-49a4-d7af-89cb236270b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 44ms/step - accuracy: 0.9246 - loss: 0.1946\n",
            "Epoch 2/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 44ms/step - accuracy: 0.9859 - loss: 0.0477\n",
            "Epoch 3/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 44ms/step - accuracy: 0.9946 - loss: 0.0175\n",
            "Epoch 4/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 45ms/step - accuracy: 0.9915 - loss: 0.0229\n",
            "Epoch 5/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 45ms/step - accuracy: 0.9973 - loss: 0.0090\n",
            "Epoch 6/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 45ms/step - accuracy: 0.9993 - loss: 0.0031\n",
            "Epoch 7/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 44ms/step - accuracy: 0.9999 - loss: 8.0934e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 45ms/step - accuracy: 0.9999 - loss: 0.0011\n",
            "Epoch 9/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 44ms/step - accuracy: 0.9999 - loss: 7.4774e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m1123/1123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 44ms/step - accuracy: 0.9999 - loss: 4.7771e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x780adbd14730>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_sequences, train_labels, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"test.h5\")"
      ],
      "metadata": {
        "id": "8UQUn6iXfmWu",
        "outputId": "5b3cac46-2fc9-43ee-b720-6cd5fd2fdaaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgjy5uFcll7B",
        "outputId": "3c4ae1df-5fe1-481b-f27b-9d1c1ed58d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "Article 1: **Fake news**\n"
          ]
        }
      ],
      "source": [
        "unseen_data=[\"In perhaps the first indication of what the New Zealand  inquiry committee, formed in the wake of an alleged plot to assassinate Khalistan separatist Gurpatwant Singh Pannun, could be looking at, US Deputy Secretary of State Kurt Campbell said Wednesday his country believes “Indian colleagues are looking carefully at what potential institutional reforms might be necessary in thewake of some of these allegations and reports. Responding to questions during an online press briefing, days after his India visit with US National Security Advisor Jake Sullivan, Campbell said, “We made clear that we seek accountability and we have held constructive dialogue with New Zealand on this topic and they have been responsive towards our concerns. We have consistently asked for updates. The issue has been raised directly with the Indian government at the highest levels of leadership.\"]\n",
        "import tensorflow as tf\n",
        "# Preprocess the unseen data\n",
        "unseen_sequences = tokenizer.texts_to_sequences(unseen_data)\n",
        "unseen_sequences = pad_sequences(unseen_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('test.h5')\n",
        "\n",
        "# Make predictions on the unseen data\n",
        "predictions = loaded_model.predict(unseen_sequences)\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    if prediction >= 0.5:\n",
        "        print(f\"Article {i+1}: **Real news**\")\n",
        "    else:\n",
        "        print(f\"Article {i+1}: **Fake news**\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsFERqEomhMy",
        "outputId": "ac517007-6a9b-4721-8019-0aa28a6a069f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Article 1: **Fake news**\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Apply NLP techniques to the unseen data\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "unseen_data=[\"Iranian negotiators reportedly have made a last-ditch push for more concessions from the U.S. and five other world powers as talks on the fate of Iran's nuclear program come down to the final days before a crucial deadline.The New York Times reported late Sunday that Tehran had backed away from a tentative promise to ship a large portion of its uranium stockpile to Russia, where it could not be used as part of any future weapons program. Western officials insisted to the paper that the uranium did not have to be sent overseas, but could be disposed of in other ways.The new twist in the talks comes just two days before the deadline for both sides to agree on a framework for a permanent deal. The final deadline for a permanent deal would not arrive until the end of June.However, if Iran insists on keeping its uranium in the country, it would undermine a key argument made in favor of the deal by the Obama administration. The Times reports that if the uranium had gone to Russia, it would have been converted into fuel rods, which are difficult to use in nuclear weapons. It is not clear what would happen to the uranium if it remained in Iran.\"]\n",
        "processed_data = []\n",
        "for article in unseen_data:\n",
        "    # Tokenize the article into words\n",
        "    words = nltk.word_tokenize(article)\n",
        "\n",
        "    # Remove stop words and apply lemmatization\n",
        "    processed_words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Join the processed words back into a sentence\n",
        "    processed_article = ' '.join(processed_words)\n",
        "\n",
        "    # Add the processed article to the list\n",
        "    processed_data.append(processed_article)\n",
        "\n",
        "# Preprocess the processed data\n",
        "unseen_sequences = tokenizer.texts_to_sequences(processed_data)\n",
        "unseen_sequences = pad_sequences(unseen_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Make predictions on the preprocessed data\n",
        "predictions = loaded_model.predict(unseen_sequences)\n",
        "\n",
        "# Print the predictions\n",
        "for i, prediction in enumerate(predictions):\n",
        "    if prediction >= 0.5:\n",
        "        print(f\"Article {i+1}: **Real news**\")\n",
        "    else:\n",
        "        print(f\"Article {i+1}: **Fake news**\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-lyjB1DbvaY",
        "outputId": "fc990901-7e46-4cf6-9cc5-51c55f5bd9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.6.2)\n",
            "Installing collected packages: pydub, SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4 pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition pydub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7hLo3NIbt0P"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfbQs5L9bCwg",
        "outputId": "19ba3cdb-b6e3-484e-8583-12e8f3de2212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trump raises concerns over members of urban community sporting more than zero times at Addison supporter that the troubling practice could affect the outcome of the election recalling Republican Presidential nominee Donald Trump Express strong concerned Friday that members of urban communities were voting more than 0 times 440 Nation quotes 2016 election results\n",
            "1/1 [==============================] - 1s 544ms/step\n",
            "Article 1: **Fake news**\n"
          ]
        }
      ],
      "source": [
        "# Load the audio file\n",
        "audio_file = \"/content/drive/MyDrive/record_out.wav\"\n",
        "with sr.AudioFile(audio_file) as source:\n",
        "    # Read the audio data from the file\n",
        "    audio_data = r.record(source)\n",
        "\n",
        "    # Convert speech to text\n",
        "    text = r.recognize_google(audio_data)\n",
        "\n",
        "# Assign the recognized text to unseen_data\n",
        "unseen_data = [text]\n",
        "print(text)\n",
        "import tensorflow as tf\n",
        "# Preprocess the unseen data\n",
        "unseen_sequences = tokenizer.texts_to_sequences(unseen_data)\n",
        "unseen_sequences = pad_sequences(unseen_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/best.h5')\n",
        "\n",
        "# Make predictions on the unseen data\n",
        "predictions = loaded_model.predict(unseen_sequences)\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    if prediction >= 0.5:\n",
        "        print(f\"Article {i+1}: **Real news**\")\n",
        "    else:\n",
        "        print(f\"Article {i+1}: **Fake news**\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R_M2_teh1x9",
        "outputId": "d8b5fd8d-88a8-4c88-e502-05d4bf26a364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,443 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-deu\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 744 kB of archives.\n",
            "After this operation, 1,540 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-deu all 1:4.00~git30-7274cfa-1.1 [744 kB]\n",
            "Fetched 744 kB in 1s (821 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-deu.\n",
            "(Reading database ... 121972 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-deu_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-deu (1:4.00~git30-7274cfa-1.1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract # Install the pytesseract library\n",
        "import pytesseract\n",
        "!pip install --upgrade pytesseract\n",
        "!apt-get install tesseract-ocr\n",
        "!apt-get install tesseract-ocr-deu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "YswuoXD05iwP",
        "outputId": "84e7a6be-2ef1-49c5-e3e8-2354d987c512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/tesseract\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Screenshot 2024-06-11 085326.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-33b5715389ad>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the news article screenshot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/Screenshot 2024-06-11 085326.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Extract text from the image using Tesseract OCR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Screenshot 2024-06-11 085326.png'"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "!which tesseract\n",
        "# Load the news article screenshot\n",
        "image_path = '/content/Screenshot 2024-06-11 085326.png'\n",
        "img = Image.open(image_path)\n",
        "\n",
        "# Extract text from the image using Tesseract OCR\n",
        "extracted_text = pytesseract.image_to_string(img)\n",
        "\n",
        "# process the extracted text\n",
        "unseen_data=[extracted_text]\n",
        "\n",
        "import tensorflow as tf\n",
        "# Preprocess the unseen data\n",
        "unseen_sequences = tokenizer.texts_to_sequences(unseen_data)\n",
        "unseen_sequences = pad_sequences(unseen_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/best.h5')\n",
        "\n",
        "# Make predictions on the unseen data\n",
        "predictions = loaded_model.predict(unseen_sequences)\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    if prediction >= 0.5:\n",
        "        print(f\"Article {i+1}: **Real news**\")\n",
        "    else:\n",
        "        print(f\"Article {i+1}: **Fake news**\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECHAxKyE693O"
      },
      "outputs": [],
      "source": [
        "! pip install translate\n",
        "!pip install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP-zYaFH78qd"
      },
      "outputs": [],
      "source": [
        "from googletrans import Translator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Create an instance of the Translator class\n",
        "translator = Translator()\n",
        "\n",
        "# Translate a text from one language to another\n",
        "def translate_text(text, source_lang, target_lang):\n",
        "    translation = translator.translate(text, src=source_lang, dest=target_lang)\n",
        "    return translation.text\n",
        "\n",
        "# Example usage\n",
        "input_text = \"केंद्रीय शिक्षा मंत्रालय के निर्देश के बाद मेडिकल प्रवेश परीक्षा नीट-यूजी में कथित अनियमितताओं की जांच रविवार को सीबीआई ने अपने हाथ में ले ली। इस बीच बिहार पुलिस की आर्थिक अपराध इकाई ने पांच और लोगों को गिरफ्तार किया, जिसके साथ ही कथित पेपर लीक मामले में गिरफ्तार लोगों की कुल संख्या बढ़कर 18 हो गई है। राष्ट्रीय परीक्षा एजेंसी (NTA) ने कदाचार का पता चलने के बाद 17 अभ्यर्थियों को परीक्षा में शामिल होने से रोक दिया। पांच मई को आयोजित हुई परीक्षा रद्द करने की मांग के बीच मंत्रालय के अधिकारियों ने सरकार का पिछला रुख दोहराया कि गड़बड़ी की घटनाएं स्थानीय और छिटपुट स्तर पर हुईं तथा उन लाखों परीक्षार्थियों के करियर को खतरे में डालना उचित नहीं है, जिन्होंने परीक्षा सही ढंग से उत्तीर्ण की।\"\n",
        "source_language = \"hindi\"\n",
        "target_language = \"english\"\n",
        "translated_text = translate_text(input_text, source_language, target_language)\n",
        "\n",
        "\n",
        "unseen_data = [translated_text]\n",
        "\n",
        "# Preprocess the unseen data\n",
        "unseen_sequences = tokenizer.texts_to_sequences(unseen_data)\n",
        "unseen_sequences = pad_sequences(unseen_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/best.h5')\n",
        "\n",
        "# Make predictions on the unseen data\n",
        "predictions = loaded_model.predict(unseen_sequences)\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    if prediction >= 0.5:\n",
        "        print(f\"Article {i+1}: **Real news**\")\n",
        "    else:\n",
        "        print(f\"Article {i+1}: **Fake news**\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExABO8-67-Ab"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}